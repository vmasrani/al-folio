---
layout: post
title: Closing thoughts (Mauricio)
author: Mauricio Baker
date: 2020-08-07
description: The seventh and final in a series
comments: true
---

_The seventh in a series with Mauricio Baker on critical rationalism and bayesian epistemology. See [one]({% post_url 2020-07-11-mauricio_first_response %}), [two]({% post_url 2020-07-13-vaden_first_response %}), [three]({% post_url 2020-07-17-mauricio_second_response %}), [four]({% post_url 2020-07-22-vaden_second_response %}), [five]({% post_url 2020-08-06-mauricio_third_response %}), [six]({% post_url 2020-08-06-vaden_final_response %}), and [seven]({% post_url 2020-08-06-mauricio_final_response %})._

---

Thanks for your final thoughts and recommendations!

It does seem that we agree on plenty. On the areas where you’re not very interested, I want to say a little about why I am:

I think accurately identifying an ideal is valuable, even if its precise applications are often incalculable and its rough applications are often intuitive. People have lots of intuitions and arguments about how we should deal with risk, and these are contradictory enough that some of them must be wrong. With a normative ideal (to put it in CR terms), we have a good way to criticize our intuitions and arguments about risk: figuring out whether they at least roughly match the normative ideal of MEU.

For instance, by asking whether they even roughly fit the ideal of MEU, we can figure out that the following arguments are all invalid:

- “If I buy lottery tickets, I have a chance at making millions, so I should buy lottery tickets.” (failure to account for how low the probability of an outcome is)
- “COVID-19 probably won’t be that bad, so we shouldn’t initiate aggressive containment programs early” (failure to consider the magnitude of a rare outcome’s importance)
- “Most people who try to significantly improve the long-term future won’t manage to do so, so people shouldn’t try” (failure to consider the magnitude of a rare outcome’s importance)
- “We’re extremely uncertain about how much risk AI will pose in future decades, so we shouldn’t act as if it’s high” (failure to properly assign weights amidst near-cluelessness)

Without a normative ideal, my concern is that we’d be muddling through our confused or vague ideas about risk, without even knowing what we’re aiming at. Our aim is bad, but we’ll do better if we know what target to occasionally glance at.

And as far as targets go, I quite like making the vast or even infinite reverberations of our choices be the best they can be.

Apologies for throwing out new arguments in a last response--I’ll assume you’d have an excellent response :)

Thanks so much for taking part in this! You’ve also taught me a lot.
