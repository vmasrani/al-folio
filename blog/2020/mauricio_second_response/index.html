<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Vaden Masrani | Human Decision Making</title>
<meta name="description" content="Vaden Masrani's academic website. A place to link to news, recent work, and what's been interesting me of late.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2020/mauricio_second_response/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Vaden</span>   Masrani
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/random/">
                random
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Human Decision Making</h1>
    <p class="post-meta">July 17, 2020 ‚Ä¢ Mauricio Baker</p>
  </header>

  <article class="post-content">
    <p><em>The third in a series with Mauricio Baker on critical rationalism and bayesian epistemology. See <a href="/blog/2020/mauricio_first_response/">one</a>, <a href="/blog/2020/vaden_first_response/">two</a>, <a href="/blog/2020/mauricio_second_response/">three</a>, <a href="/blog/2020/vaden_second_response/">four</a>, <a href="/blog/2020/mauricio_third_response/">five</a>, <a href="/blog/2020/vaden_final_response/">six</a>, and <a href="/blog/2020/mauricio_final_response/">seven</a>.</em></p>

<hr />

<hr />

<p>One part of your response that I especially appreciated was your point about how Bayesians / EAs often rely heavily on probabilities with little backing (or none?) as if they were well supported. I‚Äôll argue that what EAs are doing that looks like this is often fine, but after reading your last response I expect to be paying more attention to how probability estimates were formed, and to what this implies for their value as information, and for the value of further information.
<!--more--></p>

<p>There‚Äôs a few points I want to skip over for now:</p>

<blockquote>
  <p><em>my main concerns don‚Äôt really hinge this‚Ä¶ I‚Äôd rather not argue about the meaning of ‚Äòexistence‚Äô if we can avoid it üôÇ</em></p>
</blockquote>

<p>Fair enough üôÇ</p>

<p>As well as:</p>

<blockquote>
  <p><em>Without [probabilities based on measurements], we are claiming knowledge where we don‚Äôt have it and this is dangerous.</em></p>

  <p><em>‚Ä¶we don‚Äôt have a total order across explanations, so can‚Äôt coherently talk about ‚Äúsecond best‚Äù, ‚Äúthird best‚Äù, etc</em></p>

  <p><em>If the <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a> dedicated itself to coming up with societal mechanisms to prevent conflicting ideas from becoming violent, I think it would go much further to achieve its goals</em>.</p>

  <p><em>But there are no objective probabilities - no measurements from which to derive probabilities - when dealing with the unconditional future.</em></p>
</blockquote>

<p>Our disagreements on these points seem downstream of another disagreement: do humans in fact make decisions using subjective credences, and should we? I agree that this is ‚Äúan important disagreement worth spending time on.‚Äù</p>

<p>To address that question, you write:</p>

<blockquote>
  <p><em>‚Ä¶What ideal rational agent would use a decision making process which starts with an infinite regress?</em></p>
</blockquote>

<blockquote>
  <p><em>Infinite regress isn‚Äôt a problem when decision theory is used in machine learning, reinforcement learning, econometrics, etc, because the actual decisions are being made by human beings. The word ‚Äúdecision‚Äù in this case is just referring to a statistical procedure of learning from data, and is not the same as the (unknown) psychological/biological phenomenon called ‚Äúdecisions‚Äù in steps 1-5.</em></p>
</blockquote>

<blockquote>
  <p><em>‚ÄúSubjective credences‚Äù are often talked about as if they are just formed in your brain automatically and allow us to avoid having to make decisions (and thus avoiding the infinite regress problem). But this isn‚Äôt true. Try it - try filling in a few entries of the conditional probability table in step 3 for example. What number are you going to write down for p(burns mouth | drinks tea) = c ? Or p(has a productive day at work | drinks coffee) = d? What are c and d? One could tally all the times one has burned their mouth drinking tea, but that is collecting measurements and switching back to objective probabilities, which we agree are legitimate.</em></p>
</blockquote>

<p>I understood your argument here as being the following. Please let me know if I misunderstood or am interpreting uncharitably.</p>

<p>Humans do not (and should not) use expected-utility-maximization (of the sort in which consequences are weighted by subjective credences), because attempting to do so requires first making decisions about one‚Äôs options, probabilistic predictions, and utility functions. This amounts to beginning with infinite regress, which is not a great way to make decisions. Computers that maximize expected value avoid this problem by having humans make these initial decisions, and then updating their probabilities by using data according to programmed rules. If humans worked like this, then we could follow decision theory without falling into infinite regress. However, humans don‚Äôt work like this‚Äìour inability to introspect on precise, numerical subjective credences is strong evidence for the claim that we do not automatically calculate subjective credences. Even if we formed subjective credences by (consciously?) examining data, we would just be following frequentist statistics, and this would not support the claim that Bayesian statistics offer further value.</p>

<p>‚ÄãMy current impression is that humans do work roughly like the computers you mention, in that we are born with (or otherwise develop, without choosing) at least a few assignments of value, credences, and ways of updating our credences based on data.</p>

<p>Could you say more about why you think this is not the case? I know very little about the relevant psychology.</p>

<p>Some reasons why I think that it is the case:</p>
<ul>
  <li>Neurons seems roughly Bayesian:
    <ul>
      <li>Neurons‚Äô firing rates vary, depending on the firing rates of input neurons, and rates are on continuums. This looks like implicitly updating probabilistic credences in response to probabilistic evidence.</li>
      <li>Neurons have varying responsiveness to other neurons. This looks like implicit beliefs about conditional probabilities‚Äìabout the strength of evidence offered by different inputs.</li>
      <li>Through Hebbian learning, neurons‚Äô responsiveness to other neurons changes. This looks like updating one‚Äôs implicit beliefs about conditional probabilities. The conditions that cause changed responsiveness (one neuron‚Äôs firing preceding that of another more or less frequently) correspond to conditions in which it is likely to be appropriate to update one‚Äôs conditional probability (if it is frequently appropriate to fire a neuron A shortly before firing a neuron B, then neuron A is likely a good indicator of when it is appropriate to fire neuron B).</li>
      <li><a href="https://www.lesswrong.com/posts/hN2aRnu798yas5b2k/a-crash-course-in-the-neuroscience-of-human-motivation#Expected_Utility_in_Neurons">Some studies</a> suggest that neurons encode, not just subjective credences, but expected utilities</li>
    </ul>
  </li>
  <li>Humans have rough access to probability assignments in the form of degrees of confidence:
    <ul>
      <li>e.g. When I think about the idea that ‚Äúit is July,‚Äù I feel very confident in it. When I think about the idea that ‚Äúall cats are secretly hamsters,‚Äù I feel very skeptical of it (and amused, but that‚Äôs less relevant).</li>
      <li>These feelings of confidence update in response to evidence, in roughly the ways that one would expect a Bayesian who relies heavily on heuristics to update their credences:
        <ul>
          <li>e.g. Back when a bunch of evidence suggested that it was June, I did not feel confident in the idea that ‚Äúit is July‚Äù</li>
          <li>Availability bias as an example of updating one‚Äôs credences with heavy reliance on heuristics</li>
        </ul>
      </li>
      <li>If these feelings aren‚Äôt subjective credences that I usually track automatically (and consciously, when I put my slow, explicit thinking to the task), then what are they?</li>
    </ul>
  </li>
  <li>You might ask: if this is the case, then why doesn‚Äôt a precise number come to mind when I consider P(burns mouth | drinks tea)? Because our [automatic, intuitive systems of thinking that often track and update credences] are much older, in evolutionary terms, than [our conscious, explicit systems of thinking that can handle abstract quantities]. And evolution is a tinkerer, so we did not develop slow, explicitly symbolic thinking by completely rewiring our automatic thinking into using explicitly symbolic terms.
    <ul>
      <li>While I lack a precise, introspectively available probability, I can still say something like ‚Äúbetween 1/100 and 1/2.‚Äù Such broad ranges are often enough to have clear implications, such as when the stakes of one possibility are very high.</li>
    </ul>
  </li>
</ul>

<p>Neuroscience and introspection, then, suggest that humans do automatically track probabilities with subjective credences. These processes mean that humans following decision theory would not begin with infinite regress‚Äìwe already have some beliefs about actions we can take and their likely consequences (as well as some preferences over outcomes), and these offer a starting point for decision.</p>

<p>Given that decision theory does not lead to infinite regress, claiming that it only constrains preferences, given some other preferences, seems to sell it short. Given beliefs and preferences, decision theory constrains actions‚Äìgiven a mapping from actions to probabilistic outcomes, and a preference order over probabilistic outcomes, there is a rational preference order over actions. If I believe that it will probably rain, and I do not want to get drenched, then it is a good idea (by my own lights) to pack an umbrella.</p>

<p>I find ‚Äúmaximize expected value‚Äù appealing as a way of making decisions under uncertainty for at least three reasons:</p>
<ol>
  <li>It offers a way to account for things I care about: the desirability of consequences, and their likelihood if I take some action</li>
  <li>It is demonstrably best in the situations where it is most feasible to judge which strategy is best: If our credences match objective probabilities exactly, then expected value equals average value over many trials. So anyone seeking to maximize total value over many trials would do best by maximizing expected value at each trial. This would still pretty much be the case if our credences closely approximated objective probabilities. As a result, any strategy that deviates from ‚Äúmaximize expected value‚Äù will do predictably worse than ‚Äúmaximize expected value,‚Äù at least in these circumstances.
-. This is, admittedly, limited to situations that involve many trials. But that seems fine‚Äìit seems unwise to accept a general strategy that does not work well if used many times.
    <ul>
      <li>I don‚Äôt know how to judge which strategy is best in other circumstance: when my subjective credences deviate significantly from objective probabilities. That seems like asking ‚Äúwhat would be the case if I‚Äôm just terrible at figuring out what is the case?‚Äù If I‚Äôm terrible at figuring out what is the case, then how would I know?</li>
    </ul>
  </li>
  <li>I know of no other general approach to making decisions with these or similarly appealing qualities</li>
</ol>

<p>I‚Äôve argued that humans have credences, that rationality constrains actions, and that maximizing expected value seems best. Next, I‚Äôll argue that the weights we should use when making decisions should draw on more than just frequentist analysis, because there are other useful measures of reality, and there is empirical support for the relative accuracy of Bayesian forecasting. After that, I‚Äôll address a few loose ends.</p>

<blockquote>
  <p><em>One could tally all the times one has burned their mouth drinking tea, but that is collecting measurements and switching back to objective probabilities, which we agree are legitimate. Calling them ‚Äúsubjective credences‚Äù doesn‚Äôt actually help us to decide anything.</em></p>
</blockquote>

<p>Drawing on credences can help us make better decisions, because credences can be usefully informed by more than just datasets:</p>
<ul>
  <li>We can sometimes make useful predictions on the basis of an event‚Äôs causal dynamics. For example, consider a symmetrical 60-sided die. I have no data on how often such dies roll the number 60. But my understanding of how it works allows me to predict that the objective probability is 1/60. This is (I think you‚Äôll agree) useful and accurate, and I could not make this prediction if I saw frequentist data as the only useful evidence for probabilistic prediction-making.
    <ul>
      <li>A lot of the research that organizations like FHI do seems to be this: efforts to understand the causal dynamics of [events about which we lack data] to inform probabilistic predictions</li>
    </ul>
  </li>
  <li>We can make inferences from somewhat-relevant reference classes (maybe frequentist analysis is fine with this?)
    <ul>
      <li>For example, one‚Äôs forecast of the likely impact of advanced AI can be informed by data about the previous impacts of new technologies, transformative technologies, and expansions in cognitive abilities. Insofar as these are similar, frequencies are likely correlated, so accounting for these will bring one‚Äôs subjective credence closer to the objective probability.</li>
    </ul>
  </li>
  <li>Bayesian updating allows for a holistic combination of multiple, different sources of evidence into a better-informed credence. I don‚Äôt see how frequentist analysis does.
    <ul>
      <li>This also allows for accounting for relevant arguments.</li>
    </ul>
  </li>
  <li>What if one doesn‚Äôt remember ‚Äúall the times one has burned their mouth drinking tea‚Äù? If this were the case, and if it were also the case that one‚Äôs automatic credence-formation had tracked those now-consiously-forgotten burnings, then one‚Äôs subjective credence would be informed by a potentially much larger dataset than a probability formed through deliberate frequentist analysis</li>
  <li>If some prior credences are heritable, then evolution would probably select for evolutionarily useful priors, which would often be useful for human goals (e.g. high credence in ‚Äúspiders are dangerous!‚Äù). Bayesian updating from one‚Äôs priors can make use of this useful information, which is not explicitly available in datasets. Another way of thinking about this might be that, because they were naturally selected for usefulness over many generations, some prior beliefs are in accord with large data sets (e.g. how often spiders are dangerous).</li>
</ul>

<p>Carefully formed subjective credences have several advantages over probabilities informed only by frequentist statistics‚Äìthey account for a wide range of predictively useful considerations: causal dynamics, arguments, and datasets that are not explicitly accessible.</p>

<p>Do our subjective credences track these measures of reality well enough to be useful? It seems that way (especially when people are careful to mitigate the inappropriate application of heuristics):</p>

<p>As <a href="https://web.archive.org/web/20200406135830/http://slatestarcodex.com/2016/02/04/book-review-superforecasting/">Scott Alexander‚Äôs review</a> of Superforecasters discusses, Philip Tetlock‚Äôs team of ‚Äúsuperforecasters‚Äù probabilistically predicted future events (the details of which were unprecedented) with greater accuracy (closeness to objective probabilities) than chance, and with greater accuracy than CIA forecasters working with classified information. (And they didn‚Äôt just get lucky‚Äìthe same people tended to be superforecasters over multiple rounds of testing.) <a href="https://web.archive.org/web/20200426202327/http://slatestarcodex.com/2016/02/07/list-of-passages-i-highlighted-in-my-copy-of-superforecasting/">Tetlock writes</a>:</p>

<blockquote>
  <p><em>The superforecasters are a numerate bunch: many know about Bayes‚Äô theorem and could deploy it if they felt it was worth the trouble. But they rarely crunch the numbers so explicitly. What matters far more to the superforecasters than Bayes‚Äô theorem is Bayes‚Äô core insight of gradually getting closer to the truth by constantly updating in proportion to the weight of the evidence. That‚Äôs true of Tim Minto [the top superforecaster]. He knows Bayes‚Äô theorem, but he didn‚Äôt use it even once to make his hundreds of updated forecasts. And yet Minto appreciates the Bayesian spirit. ‚ÄúI think it is likely that I have a better intuitive grasp of Bayes‚Äô theorem than most people,‚Äù he said, ‚Äúeven though if you asked me to write it down from memory I‚Äôd probably fail.‚Äù Minto is a Bayesian who does not use Bayes‚Äô theorem. That paradoxical description applies to most superforecasters.</em></p>
</blockquote>

<p>Assuming that Tetlock‚Äôs claim is supported by the thousands of data points he collected, frequentist analysis seems to provide support for the usefulness of Bayesian analysis. More generally, this is frequentist evidence for the capacity of people to make better-than-random forecasts of events without information about previous trials of the events.</p>

<p>Tetlock found that several other factors <a href="https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/">were correlated</a> with accuracy, including well-informedness, intelligence, deliberation time, belief updating, training on forecasting skills, team-based forecasting, and questioning one‚Äôs initial intuitions. This looks pretty good for Toby Ord, and for thinking that such forecasts are far from probabilities ‚Äúmade up out of thin air.‚Äù</p>

<p>Note that the predictions that Tetlock‚Äôs forecasters made were unconditional probabilistic predictions. And the vast majority of forecasters <a href="https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/">did better than chance</a> (not just the superforecasters, although they did especially better than chance). This is (frequentist) evidence against the claim that</p>

<blockquote>
  <p><em>there are no objective probabilities - no measurements from which to derive probabilities - when dealing with the unconditional future.</em></p>
</blockquote>

<p>Other implications for Bayesian analysis:</p>

<p>You write that:</p>

<blockquote>
  <p><em>The probability calculus is so powerful because it can be used to learn about the world by abstracting and aggregating information from complex phenomena which are often too difficult to analyze by other means. But this always requires there to be a dataset - measurements of reality - behind these calculations. Without this, we are claiming knowledge where we don‚Äôt have it and this is dangerous.</em></p>
</blockquote>

<p>I agree, and I think that this is an important point. I would add that, as I have argued, extensive datasets of highly relevant reference classes are not the only useful measures of reality‚Äìlogical arguments, information about causal dynamics, somewhat-relevant reference classes, and datasets that can only be accessed by considering our subjective credences also measure reality, and our subjective credences (more or less) successfully account for these. Our subjective credences are erroneous enough that I‚Äôm happy to act in accordance with solid frequentist data when we have it. When we don‚Äôt, our subjective credences still offer useful measures of reality. Without these, we are acting as if we have ignorance where we don‚Äôt have it, and this, too, is dangerous.</p>

<blockquote>
  <p><em>‚Ä¶we don‚Äôt have a total order across explanations, so can‚Äôt coherently talk about ‚Äúsecond best‚Äù, ‚Äúthird best‚Äù, etc</em></p>
</blockquote>

<p>If explanations are particular claims about what the world is like, and if, as I have argued, we have partial credences in such claims, then we can coherently talk about second best explanations, and thus make use of them.</p>

<p>To address a few other points:</p>

<blockquote>
  <p>_ There is a result in epistemology which proves that certain events are not knowable in principle - namely those events which are causally dependent on future knowledge, which we by definition don‚Äôt have_
<em>[‚Ä¶]</em>
<em>It‚Äôs important to recognize that this result only applies to unconditional historical predictions (‚ÄúX will happen‚Äù.) but not conditional ones (i.e. ‚ÄúIf X happens, then Y will happen‚Äù).  Relevant to our discussion, it also applies to probabilistic unconditional predictions (‚ÄúX has a Y% chance of happening‚Äù), for to assign probabilities would require us to put a distribution over a set of events we do not know.</em></p>
</blockquote>

<p>I like this argument‚Äìvery neat result. Your last step here, going from ruling out definite predictions to ruling out probabilistic forecasts, doesn‚Äôt make sense to me. We cannot predict all (or much) of how things will go in the future, but we are not completely clueless either‚Äìwe can still make useful probabilistic forecasts, on the basis of aspects of the future about which we are not clueless.</p>

<blockquote>
  <p><em>One could try to get out of this by switching to a subjectivistist view probability, and rely instead on the ‚Äúcredences of experts‚Äù. But this is arbitrary. How do we select which experts to listen to? How can we ignore experts in theology‚Ä¶?</em></p>
</blockquote>

<p>I agree that using expert credences requires us to make choices. Why would it follow that these choices must be arbitrary? A few ways in which the choice of which experts to listen to may be made non-arbitrarily:</p>

<ul>
  <li>Use frequentist data about experts. Tetlock <a href="https://web.archive.org/web/20200406135830/http://slatestarcodex.com/2016/02/04/book-review-superforecasting/">also</a> provides evidence for the claim that experts who consider many angles / perspectives do better than chance at forecasting, while experts who ‚Äúknow one big thing‚Äù tend to do worse than chance. My limited knowledge of (especially Abrahamic) theology suggests that ‚Äúknowing one big thing‚Äù is a decently accurate characterization of much of it.</li>
  <li>Theologians predicting the end of the world also have a pretty terrible track record</li>
  <li>Use your holistically informed, subjective credences about the reliability of various kinds of experts. My subjective credences say that intellectual traditions which valorize faith will tend to produce predictions less correlated with objective probabilities.
    <ul>
      <li>Yes, this argument for subjective credences is circular. This is fine, because here I‚Äôm arguing, not that Bayesianism has external justification, but that it‚Äôs coherent.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><em>All we can ever do is try to criticize the best explanation through arguments and experiments, and try to come up with better ones.</em></p>
</blockquote>

<p>And then how do you make decisions? I‚Äôm still confused about what the decision-making alternative to [something that‚Äôs roughly expected-value-maximization] is.</p>

<h3 id="update">Update</h3>
<p><em>(Added a few days later)</em></p>

<p>I came across some material that surprised me, so I want to add a correction to my response:</p>

<p>Reading a little more Tetlock, it looks like he makes a major caveat that I hadn‚Äôt been aware of: ‚Äúthere is no evidence that geopolitical or economic forecasters can predict anything ten years out beyond the excruciatingly obvious‚Ä¶ These limits on predictability are the predictable results of the butterfly dynamics of nonlinear systems. In my EPJ research, the accuracy of expert predictions declined toward chance five years out.‚Äù</p>

<p>While this finding is not directly about technological forecasts, it‚Äôs suggestive that my previous optimism about them doesn‚Äôt hold up at all. I‚Äôm not sure what to make of this. Maybe:</p>

<p>Expected-value maximization still makes sense, although now the best way of going about it for long-term predictions involves overriding one‚Äôs automatic/intuitive formation of credences, and giving all non-negligible possibilities equal weight, except when one has very strong reasons to move away from equal weights.</p>

<p>Starting with a prior probability of 50% makes sense if one is completely clueless. Using the broadest reference class‚Äìall possible predictions‚Äìhalf of all possible predictions are correct, as for every possible prediction that is correct (‚ÄúA will happen‚Äù), there is a possible prediction that is incorrect (‚ÄúA will not happen‚Äù). (This feels sketchy.)</p>

<p>Given quite strong evidence, one should adjust one‚Äôs estimate significantly (since Tetlock does grant that people can predict ‚Äúthe excruciatingly obvious‚Äù):</p>
<ul>
  <li>If some categories of things that may happen are broader than others (e.g. ‚Äúhuman extinction‚Äù vs ‚Äúhuman extinction through nuclear war‚Äù), one can (and mathematically should) give greater weight to broader categories.</li>
  <li>Looking back to Ord‚Äôs estimates: frequentist evidence is strong evidence for adjusting estimate of non-anthropogenic risks way down, while climate models + ancient history arguably make it obvious that, while climate change and nuclear war pose enormous risks, they do not significantly threaten human extinction.</li>
  <li>So this line of reasoning seems to generally support Ord‚Äôs low estimates, while suggesting that his higher estimates (e.g. risks from AI) should have stayed even closer to the clueless prior of 50%</li>
</ul>

  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'vmasrani';
      var disqus_identifier = '/blog/2020/mauricio_second_response';
      var disqus_title      = "Human Decision Making";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2020 Vaden Masrani.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: August 08, 2020.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
